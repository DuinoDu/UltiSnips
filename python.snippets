####################################
#
# os, file, folder, read, write
#
####################################

snippet isdir
if not os.path.exists(${1:dirname}):
    os.makedirs($1)
endsnippet

snippet join
os.path.join($1)
endsnippet

snippet dump
with open('${2:file}', 'w') as fid:
	pickle.dump(${1:var}, fid)
endsnippet

snippet load
with open('${1:file}', 'r') as fid:
	${2:var} = pickle.load(fid)
endsnippet

snippet getimg
${1:imgfiles} = sorted([os.path.join(${2:root}, x) for x in sorted(os.listdir($2)) if x.endswith('.jpg')])
endsnippet

snippet readxml
#import xml.etree.ElementTree as ET
tree = ET.parse(xmlfile)
width = int(tree.find('size').find('width').text)
height = int(tree.find('size').find('height').text)
objs = tree.findall('object')
for index, obj in enumerate(objs):
    name = obj.find('name').text.lower()
    bbox = obj.find('bndbox')
    x1 = int(bbox.find('xmin').text) - 1
    y1 = int(bbox.find('ymin').text) - 1
    x2 = int(bbox.find('xmax').text) - 1
    elif args.cuda:
        net = net.cuda()
    y2 = int(bbox.find('ymax').text) - 1
endsnippet

snippet sh
try:
    import commands
except Exception as e:
    import subprocess as commands

cmd = '$1'
(status, output) = commands.getstatusoutput(cmd)
output = output.split('\n')
endsnippet

snippet addpath
import os, sys
sys.path.append(os.path.join(os.path.dirname(__file__), "${1:..}"))
endsnippet

snippet sort
${1:x} = sorted($1, key=lambda ${2:input} : $2[${3:"keyname"|0}])
endsnippet

snippet re
import re
m = re.search('${1:reg pattern}', ${2:str})
if m is not None:
	output = m.group(0)
else:
	pass
endsnippet

####################################
#
# display, show 
#
####################################

snippet print
print($1)
endsnippet

snippet printi
    sys.stdout.flush()
    sys.stdout.write('writing {}/{}\r'.format(i, len(files)))
    i += 1 # set i = 0 before looping
print '\nFinish!'
endsnippet

snippet printcolor "print colorful string" b
from termcolor import cprint  
cprint(i, 'green', 'on_red', end=' ') 
endsnippet

snippet printtable "print table on terminal" b
from prettytable import PrettyTable
table = PrettyTable(['${1:name1}', '${2:name2}'])
table.add_row([${3:name1_value}, ${4:name2_value}])
table.sort_key($2)
table.reversesort = True
print(table)
endsnippet

snippet progress
import tqdm
t = tqdm.tqdm()
t.total = ${1:length}
for i in range($1):
    t.update()
endsnippet


####################################
#
# PIL 
#
####################################

snippet topil
#from PIL import Image
#import cv2
${1:im} = Image.fromarray(cv2.cvtColor($1,cv2.COLOR_BGR2RGB))
endsnippet

snippet tocv2
#from PIL import Image
#import numpy as np
${1:im} = np.array($1)[:,:,::-1].copy()
endsnippet


####################################
#
# cv2 
#
####################################

snippet camera
cap = cv2.VideoCapture(0)
while True:
    ok, f = cap.read()
    if ok:    
        cv2.imshow("image", f)
    if cv2.waitKey(20) == 27:
        break
cv2.destroyAllWindows()
endsnippet

snippet esc
cv2.imshow('img', img)
ch = cv2.waitKey(0) & 0xff
if ch == 27: #ord('q')
	break
endsnippet

snippet cv2rect
${1:img} = cv2.rectangle($1, (${2:x1}, ${3:y1}), (${4:x2}, ${5:y2}), (0, 255, 0), 2)
endsnippet
snippet cv2rect
${1:img} = cv2.rectangle($1, (${2:pt}[0], $2[1]), ($2[2], $2[3]), (0, 255, 0), 2)
endsnippet
snippet cv2circle
${1:img} = cv2.circle($1, (${2:x0}, ${3:y0}), ${4:r}, (0, 255, 0), 2)
endsnippet
snippet cv2resize
${1:img} = cv2.resize($1, (int($1.shape[1]/${2}), int($1.shape[0]/$2)))
endsnippet
snippet cv2text
${1:img} = cv2.putText($1, ${2:str},(${3:x}, ${4:y}),cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)
endsnippet
snippet cv2line
${1:img} = cv2.line($1, (${2:x1}, ${3:y1}), (${4:x2}, ${5:y2}), (0, 255, 0), 2)
endsnippet

snippet contour
im = cv2.imread(${1:imgpath})
im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
_, im_thresh = cv2.threshold(im_gray, 177, 255, cv2.THRESH_BINARY)
_, contours, hierarchy = cv2.findContours(im_thresh, cv2.RETR_TREE ,cv2.CHAIN_APPROX_SIMPLE)
cv2.drawContours(im, contours,-1,(0,255,0),1)
endsnippet


####################################
#
# numpy
#
####################################

snippet tohwc
${1:ndarray} = np.transpose(${2:ndarray}, (1,2,0))
endsnippet

snippet tochw
${1:ndarray} = np.transpose(${2:ndarray}, (2,0,1))
endsnippet


####################################
#
# pytorch
#
####################################

snippet totensor
${1:tensor} = torch.from_numpy(${2:ndarray}.transpose((2, 0, 1))).float().div(255)
endsnippet

snippet tonumpy
${1:ndarray} = torch.cpu().numpy(${2:tensor})
endsnippet

snippet inctorch
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch.nn.init as init
from torch.autograd import Variable
import torch.utils.data as data
endsnippet

snippet loadgan
parser.add_argument('--resume', default="", type=str, help='resume weight pth path')
if args.resume != "":
	net.load_state_dicts(torch.load(args.resume))
endsnippet

snippet checkpoint
parser.add_argument('--save_folder', default="weights", type=str, help='save weight pth path')
if not os.path.exists(args.save_folder):
    os.makedirs(args.save_folder)

torch.save(net.state_dict${1:s}(), os.path.join(args.save_folder, 'weights.pth'))
endsnippet

snippet setgpu
parser.add_argument('--cuda', default=1, type=int, help='if using cuda')
parser.add_argument('--gpuID', default=0, type=int, help='Use which gpu to train model')
import os
os.environ['CUDA_VISIBLE_DIVICES'] = str(args.gpuID)
if args.cuda:
	torch.cuda.set_device(args.gpuID)
endsnippet
snippet multigpu
${1:net} = nn.DataParallel($1).cuda()

endsnippet

snippet dataset
import torch.utils.data as data

class ${1:dataset name}(data.Dataset):
    """${2: description}

    Arguments:
        root (str): root of dataset.
		split (str): 'train' | 'val' | 'trainval'
    """

    def __init__(self, root, split, transform=None, target_transform=None):
        self.root = root
        self.image_set = split
        self.transform = transform # data-aug
		self.target_transform = target_transform
        self.name = ${3:dataset_name}
        self._imgfile = ${4:image path}
        self._annofile = ${5:annotation file}
        self.ids = list()
		# TODO: add self.ids

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, index):
		# TODO: fetch [x, y]
        #return im, gt
		pass


if __name__ == '__main__':
	import os
	root = os.path.join(os.environ['HOME'], 'data/VOCdevkit')
	ds = $1(root)
	print("dataset len :", len(ds))

	import IPython
	IPython.embed()
endsnippet

snippet testds
if __name__ == '__main__':
	import os
	root = os.path.join(os.environ['HOME'], 'data/VOCdevkit')
	ds = ${1:dataset name}(root)
	print("dataset len :", len(ds))

	import IPython
	IPython.embed()
endsnippet

snippet dataloader
# add to argparse
def str2bool(i):
	return i.lower() in ['y','t','yes','true','1']
parser.add_argument('--batch_size', default=16, type=int, help='batch size for train and val')
parser.add_argument('--num_workers', default=2, type=int, help='num of workers to load data')
parser.add_argument('--shuffle', default='y', type=str2bool, help='shuffle data when loading data')
parser.add_argument('--pin_memory', default='y', type=str2bool, help='pin memory when loading data')
parser.add_argument('--num_epochs', default=10, type=int, help='num of epochs')


dataset = ${1:dataset_name}(${2:args})
${3:train|val}loader = data.DataLoader(dataset, args.batch_size, num_workers=args.num_workers, shuffle=args.shuffle, pin_memory=args.pin_memory)

for epoch in range(args.num_epochs):
	for i, data in $3loader:
		pass
endsnippet

snippet addtran
if self.transform != None:
	im = self.transform(im)
if self.target_transform != None:
	im, gt = self.target_transform(im, gt)
endsnippet

snippet tran
class ${1:name}(object):
    def __init__(self, ${2:args}):
		self.$2 = $2
		pass
    def __call__(self, im, ${3:args}):
		pass
endsnippet

snippet compose
class ${1:name}Augmentation(object):
	def __init__(self, ${3:args}):
		self.transforms = [
				# TODO: add more
				ToTensor(),
				]
	def __call__(self, ${2:im}):
		for t in self.transforms:
			$2 = t($2)
		return $2
endsnippet

snippet model
import torch
import torch.nn as nn
import torch.nn.init as init

class ${1:name}(nn.Module):

    def __init__(self${2:, n_classes}${3:, phase}${4:, pretrain=False}):
        super($1, self).__init__()
		self.$2 = $2
		self.$3 = $3

        # TODO: design model

    def forward(self, x):
        # TODO: build model
        return x

    def init_weight(self, weight_file=None):
        pass

if __name__ == "__main__":
    from torch.autograd import Variable
    net = $1($2, $3, $4)
    x = Variable(torch.randn(1,3,1000,800))
    pred = net(x)
    print(pred.size())

    import IPython
    IPython.embed()
endsnippet

snippet testmodel
if __name__ == "__main__":
    from torch.autograd import Variable
    net = ${1:Model}(2)
    x = Variable(torch.randn(1,3,1000,800))
    pred = net(x)
    print("input size: ", x.size())
    print("output size:", pred.size())

    import IPython
    IPython.embed()
endsnippet

snippet basenet
	self.${1:base}_net = self._$1_net()

def _$1_net(self):
	pass
endsnippet

snippet initweight
    self._init_weight()

def _init_weight(self):
    def weight_init(m):
        if isinstance(m, nn.Conv2d):
            init.${1:xavier_uniform}(m.weight.data)
            if m.bias is not None:
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(${2:1})
            if m.bias is not None:
                m.bias.data.zero_()
    self.apply(weight_init)
endsnippet

snippet loadweight
    if pretrain:
        self._load_weight()

from torchvision import models
import os, glob
def _load_weight(self, weight_file=None):

    def _fetch_weight():
        print('Fetching pretrained model...')
        vgg16 = models.vgg16(pretrained=True)
        model_file = os.path.join(os.environ['HOME'], '.torch/models', 'vgg16-*.pth')
        return glob.glob(model_file)[0]

    if weight_file == None:
        weight_file = _fetch_weight()

    _, ext = os.path.splitext(weight_file)
    if ext not in ('.pkl', '.pth'):
        raise ValueError, 'Sorry, only .pth and .pkl are supported, get {}'.format(weight_file)

    source_dict = torch.load(weight_file)
    # features -> base_net, remove
    target_dict = {}
    for key in source_dict.keys():
        # TODO
        pass
    source_dict = target_dict 
    # add
    for (key, value) in self.state_dict().items():
        if key not in target_dict.keys():
            target_dict[key] = value
    self.load_state_dict(target_dict)
    print('Loading weight successfully!')
endsnippet

snippet initarg
import argparse
parser = argparse.ArgumentParser(description='${1:description of this script}')
args = parser.parse_args()
endsnippet

snippet addarg
parser.add_argument('--${1:name}', default=${2}, type=${3:str}, help='${4:description}')
endsnippet

snippet optim
parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, help='initial learning rate')
parser.add_argument('--momentum', default=0.9, type=float, help='momentum')
parser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay for SGD')
parser.add_argument('--gamma', default=0.1, type=float, help='Gamma update for SGD')

optimizer = torch.optim.SGD(${1:model}.parameters(), args.lr,
                            momentum=args.momentum,
                            weight_decay=args.weight_decay)
endsnippet

snippet lr
import torch.optim.lr_scheduler as lr_scheduler
lr_sche = lr_scheduler.StepLR(optimizer, step_size=30, gamma=args.gamma)
# insert to train loop
lr_sche.step()
endsnippet

snippet summary
parser.add_argument('--tensorboard', default=1, type=int, help='Use tensorboard for loss visualization')
if args.tensorboard:
   from tensorboardX import SummaryWriter
   writer = SummaryWriter()
# insert to train loop
if args.tensorboard:
Â¦   writer.add_scalar('${1:train_loss/cls}', ${2:loss.data[0]}, ${3:iteration})
writer.close()
endsnippet


####################################
#
# mxnet
#
####################################
snippet testsym
data = mx.sym.Variable(name='data')
data_shape = (1, 3, 256, 256)
data = mx.sym.Variable(name='data')
net_symbol = ${1:build_net}(data)
output_shape = net_symbol.infer_shape(data=data_shape)
print('network output shape:', output_shape[1])
endsnippet

snippet testblock
x = nd.random_normal(shape=(${1:1, 3, 224, 224}))
net = ${2:Net}()
net.collect_params().initialize()
y = net(x)
print(y.shape)
endsnippet

snippet symconv 
${3:output} = mx.symbol.Convolution(name='${1:name}', data=${2:input}, num_filter=${4:64}, pad=(${5:1}, $5), 
                                    kernel=(${6:3}, $6), stride=(${7:1}, $7), no_bias=${8:True})
endsnippet

snippet symbn
${3:output} = mx.symbol.BatchNorm(name='${1:name}', data=${2:input}, use_global_stats=True, fix_gamma=False, 
                                  eps=${4:eps})
endsnippet

snippet symrelu
${3:output} = mx.symbol.Activation(name='${1:name}', data=${2:input}, act_type='relu')
endsnippet

snippet sympool
${3:output} = mx.symbol.Pooling(name='${1:name}', data=${2:input}, pooling_convention='full', 
                                pad=(${4:0}, $4), kernel=(${5:3}, $5), stride=(${6:2}, $6), pool_type='${7:max}')
endsnippet

snippet incgluon
import mxnet as mx
import mxnet.ndarray as nd
import mxnet.gluon as gluon
import mxnet.gluon.nn as nn
import mxnet.autograd as autograd
endsnippet

snippet hs "gluon.nn.HybridSequential"
${1:out} = nn.HybridSequential(${2:**kwargs})
with $1.name_scope():
	$1.add($3)
endsnippet

snippet hs2 "gluon.nn.HybridSequential"
with self.name_scope():
	${1:out} = nn.HybridSequential()
	$1.add($2)
endsnippet

snippet hb "gluon.nn.HybridBlock"
class ${1:BlockName}(nn.HybridSequential):
	def __init__(self, ${2}, **kwargs):
		super($1, self).__init__(**kwargs)

	def hybrid_forward(self, F, x):
		${3:#TODO}
		return x
endsnippet

snippet gconv
nn.Conv2D(${1:out_channels}, ${2:kernel_size}, strides=${3}, padding=${4:1}, use_bias=${5:False})
endsnippet

snippet gbn
nn.BatchNorm(scale=True)
endsnippet

snippet tosym
sym = ${1:net}(mx.sym.var('data'))
endsnippet


snippet mxviz
mx.viz.plot_network(symbol=${1:symbol}, shape={'data':${2:(1,3,224,224)}}, title='symbol', save_format='jpg').render()
endsnippet

snippet loader
import numpy as np
import mxnet as mx

class ${1:CustomLoader}(mx.io.DataIter):
    def __init__(self, batch_size=1, ctx=None):
        super($1, self).__init__()

        # custom params
        self.batch_size = batch_size
        self.ctx = ctx

        # size and index
        self.size = ${2:dataset size}
        self.index = np.arange(self.size)

        # data and label
		# change to your dataset
        self.data_name = ['data']
        self.label_name = ['label', 'bbox_target', 'bbox_weight']

        # status variable
        self.cur = 0
        self.batch = None
        self.data = None
        self.label = None

        # get first batch to fill in provide_data and provide_label
        self.reset()
        self.get_batch()

    def reset(self):
        self.cur = 0

    def get_batch(self):
        # put data and label to self.data and self.label
        # [B, C, H, W]
        self.data = [mx.nd.zeros((1, 3, 256, 256)) for key in self.data_name]
        self.label = [mx.nd.zeros((10,10)) for key in self.label_name]

    @property
    def provide_data(self):
        return [(k, v.shape) for k, v in zip(self.data_name, self.data)]

    @property
    def provide_label(self):
        return [(k, v.shape) for k, v in zip(self.label_name, self.label)]

    def next(self):
        pad = self.cur + self.batch_size - self.size \
                if (self.cur + self.batch_size > self.size) else 0
        if self.cur + self.batch_size <= self.size:
            self.get_batch()
            self.cur += self.batch_size
            index = self.cur / self.batch_size
            return mx.io.DataBatch(data=self.data,
                                   label=self.label,
                                   pad=pad,
                                   index=index,
                                   provide_data=self.provide_data,
                                   provide_label=self.provide_label)
        else:
            raise StopIteration
endsnippet


####################################
#
# tensorflow 
#
####################################

#snippet place
#tf.placeholder(tf.float32, shape=[${1}], name='${2}')
#endsnippet
#snippet var
#tf.Variable(${1})
#endsnippet
#snippet conv
#${1} = tf.nn.conv2d(x, W, strides=[1, ${2:stride}, $2, 1], padding='SAME')
#endsnippet
#snippet pool
#${1} = tf.nn.max_pool(x, ksize=[1, ${2:size}, $2, 1], strides=[1,$2, $2, 1], padding='SAME')
#endsnippet
#snippet bias
#tf.nn.bias_add(${1})
#endsnippet
#snippet relu
#tf.nn.relu(${1})
#endsnippet
#snippet drop
#tf.nn.dropout(${1}, dropout)
#endsnippet
#snippet cross
#tf.nn.softmax_cross_entropy_with_logits(${1:model_y}, ${2:y})
#endsnippet
#snippet add
#tf.add(${1})
#endsnippet
#snippet mul
#tf.matmul(${1})
#endsnippet
#snippet rand
#tf.random_normal([${1:h}, ${2:w}, ${3:in}, ${4:out}])
#endsnippet
#snippet mean
#tf.reduce_mean(${1})
#endsnippet
#snippet cast
#tf.cast(${1}, ${2:tf.float32})
#endsnippet
#snippet equal
#tf.equal(${1})
#endsnippet
#snippet argmax
#tf.argmax(${1}, ${2:dimension})
#endsnippet
#snippet opti
#tf.train.GradientDescentOptimizer(learning_rate=${1:learning_rate}).minimize(${2:cost_op})
#endsnippet
#snippet opti
#tf.train.AdamOptimizer(learning_rate=${1:learning_rate}).minimize(${2:cost_op})
#endsnippet
#snippet runtrain
#sess.run(train_op, feed_dict = {${1:x}:${2}, ${3:y}:${4}})
#endsnippet
#snippet runcost
#sess.run(cost_op, feed_dict = {${1:x}:${2}, ${3:y}:${4}})
#endsnippet
#snippet runacc
#sess.run(accuracy_op, feed_dict = {${1:x}:${2}, ${3:y}:${4}})
#endsnippet
#snippet gpusize
#config = tf.ConfigProto()
#config.gpu_options.per_process_gpu_memory_fraction = ${1:0.5}
##with tf.Session(config=config) as sess:
#endsnippet


####################################
#
# framework
#
####################################

snippet tool
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import print_function
import argparse
import os

def $2(args):
	${3:pass}

def main(args):
    $2(args)

if __name__ == "__main__":
	parser = argparse.ArgumentParser(description='${1:description of this script}')
	# addarg here
	
	args = parser.parse_args()
	main(args)
endsnippet


snippet train
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import numpy as np

# parameters


# data (dsX, dsY)


# model


# cost


# optimizer


# summary


# train


# evaluate
endsnippet



snippet ask
import six
if six.PY3:
    str_compat = str
else:
    str_compat = unicode
def ask(question, answer=str_compat, default=None, l=None):
    def _input_compat(prompt):
        if six.PY3:
            return input(prompt) 
        else:
            return raw_input(prompt)

    if answer == str_compat:
        r = ''
        while True:
            if default:
                r = _input_compat('> {0} [{1}] '.format(question, default))
            else:
                r = _input_compat('> {0} '.format(question, default))

            r = r.strip()

            if len(r) <= 0:
                if default:
                    r = default
                    break
                else:
                    print('You must enter something')
            else:
                if l and len(r) != l:
                    print('You must enter a {0} letters long string'.format(l))
                else:
                    break
        return r
    elif answer == bool:
        r = None
        while True:
            if default is True:
                r = _input_compat('> {0} (Y/n) '.format(question))
            elif default is False:
                r = _input_compat('> {0} (y/N) '.format(question))
            else:
                r = _input_compat('> {0} (y/n) '.format(question))

            r = r.strip().lower()

            if r in ('y', 'yes'):
                r = True
                break
            elif r in ('n', 'no'):
                r = False
                break
            elif not r:
                r = default
                break
            else:
                print("You must answer 'yes' or 'no'")
        return r
    elif answer == int:
        r = None
        while True:
            if default:
                r = _input_compat('> {0} [{1}] '.format(question, default))
            else:
                r = _input_compat('> {0} '.format(question))

            r = r.strip()

            if not r:
                r = default
                break

            try:
                r = int(r)
                break
            except:
                print('You must enter an integer')
        return r
    else:
        raise NotImplemented(
            'Argument  must be str_compat, bool, or integer')

# Usage:
# value = ask('question?', str_compat, 'default')
# value = ask('question?', int, 0)
# value = ask('question?', bool, default)
endsnippet

####################################
#
# tool 
#
####################################

snippet tic
import time
t1 = time.time()
${1:add your function}
t2 = time.time()    
print("%.2f ms" % ((t2-t1)*1000))
endsnippet

snippet pp
ncpus = 12
ppservers = ()
job_server = pp.Server(ncpus, ppservers=ppservers)

inputs = []
def ${1:foo_work}(${2:arg}):
	# add yor for-loop here 

start_time = time.time()
print("Starting pp with", job_server.get_ncpus(), "workers")
jobs = [(input, job_server.submit($1, (input,)) for input in inputs]
for job in jobs():
	job()
print "Time elapsed: ", time.time() - start_time, "s"

job_server.print_stats()
endsnippet

snippet unittest
import unittest

class ${1:MySymbol}(unittest.TestCase):

	def test_${2:fname}(self):
		pass

if __name__ == '__main__':
    unittest.main()
endsnippet

snippet skip
@unittest.skip("tested")
endsnippet

snippet testeq
self.assertEqual(${1:v1}, ${2:v2})
endsnippet

snippet testtrue
self.assertTrue(${1:bool})
endsnippet

snippet timer
#import utils
timer = utils.Timer()
timer.start()
for i in range(1):
    # fun1
    timer.tictoc('${1:name}')
    # fun2
    timer.tictoc('${2:name}')
timer.end()
timer.log(show=True)
endsnippet
