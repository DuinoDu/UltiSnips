####################################
#
# numpy
#
####################################
snippet tohwc
${1:ndarray} = np.transpose(${2:ndarray}, (1,2,0))
endsnippet

snippet tochw
${1:ndarray} = np.transpose(${2:ndarray}, (2,0,1))
endsnippet

snippet npconcat
np.concatenate((${1:a}, ${2:b}), axis=${3:0})
endsnippet

snippet npunsqu
${1:a}[np.newaxis, :]
#np.expand_dims(${1:a}, axis=${2:0})
endsnippet

snippet totensor
${1:tensor} = torch.from_numpy(${2:ndarray}.transpose((2, 0, 1))).float().div(255)
endsnippet

snippet tonumpy
${1:ndarray} = torch.cpu().numpy(${2:tensor})
endsnippet

snippet shownp
from PIL import Image
Image.fromarray(${1:arr}).show()
endsnippet

snippet savepil
from PIL import Image
Image.fromarray(${1:arr}).save(${2:filename})
endsnippet

snippet npzero
np.zeros(($1), dtype=np.float32)
endsnippet

snippet npone
np.ones(($1), dtype=np.float32)
endsnippet


####################################
#
# common
#
####################################
snippet initarg
import argparse
parser = argparse.ArgumentParser(description='${1:description of this script}')
args = parser.parse_args()
endsnippet

snippet addarg
parser.add_argument('--${1:name}', default=${2}, type=${3:str}, help='${4:description}')
endsnippet

snippet addact
parser.add_argument('--${1:name}', dest='$1', action='store_${2:true}', help='${3:description}')
endsnippet


####################################
#
# pytorch
#
####################################

snippet ipdb2
import os
if os.getenv("LOCAL_RANK", None) == "0": __import__('ipdb').set_trace()
endsnippet

snippet inctorch
import torch
import torch.nn as nn
import torch.nn.init as init
import torch.backends.cudnn as cudnn
endsnippet

snippet setgpu
parser.add_argument('--cuda', default=1, type=int, help='if using cuda')
parser.add_argument('--gpuID', default=0, type=int, help='Use which gpu to train model')
import os
os.environ['CUDA_VISIBLE_DIVICES'] = str(args.gpuID)
device = torch.device('cuda' if args.cuda else 'cpu')
endsnippet

snippet module
class ${1:name}(nn.Module):

    def __init__(self, $2):
        super($1, self).__init__()
		pass

    def forward(self, x):
		pass
endsnippet

snippet testmodule
net = ${1:Model}(2)
x = torch.randn(1,3,1000,800)
pred = net(x)
print("input size: ", x.size())
print("output size:", pred.size())
endsnippet

snippet initweight
    self._init_weight()

def _init_weight(self):
    def weight_init(m):
        if isinstance(m, nn.Conv2d):
            init.${1:xavier_uniform}(m.weight.data)
            if m.bias is not None:
                m.bias.data.zero_()
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(${2:1})
            if m.bias is not None:
                m.bias.data.zero_()
    self.apply(weight_init)
endsnippet

snippet loadweight
    if pretrain:
        self._load_weight()

from torchvision import models
import os, glob
def _load_weight(self, weight_file=None):
    source_dict = torch.load(weight_file)
    source_dict = target_dict 
    self.load_state_dict(target_dict)
    print('Loading weight successfully!')
endsnippet

snippet conv
nn.Conv2d(${1:in_channels}, ${2:out_channels}, kernel_size=${3:3}, padding=${4:1})
endsnippet

snippet bn
nn.BatchNorm2d(${1::in_channels})
endsnippet

snippet relu
nn.ReLU()
endsnippet

snippet linear
nn.Linear(${1:in_channels}, ${2:out_challens})
endsnippet

snippet toonnx
torch.onnx.export(torch_model,               # model being run
				  x,                         # model input (or a tuple for multiple inputs)
				  "super_resolution.onnx",   # where to save the model (can be a file or file-like object)
				  export_params=True,        # store the trained parameter weights inside the model file
				  opset_version=10,          # the ONNX version to export the model to
				  do_constant_folding=True,  # whether to execute constant folding for optimization
				  input_names = ['input'],   # the model's input names
				  output_names = ['output'], # the model's output names
				  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
								'output' : {0 : 'batch_size'}},
				)
endsnippet

snippet dataset
from torch.utils.data import Dataset
from torchvision import transforms

class ${1:Name}(Dataset):

    def __init__(self):
		pass

    def __len__(self):
		return 0

    def __getitem__(self, i):
		pass


def test_dataset():
    dataset = $1()
    assert len(dataset)
    item = dataset[0]
endsnippet

snippet check_data
from torch.utils.data import DataLoader                                    
data_loader = DataLoader(                                                  
	data_module["train_dataset"],                                      
	batch_size=16,                                                     
	shuffle=True,                                                      
	num_workers=4,                                                     
	collate_fn=data_module["data_collator"],                           
)                                                                  
import tqdm                                                                                                                                                                                                                          
t = tqdm.tqdm(total=len(data_loader), desc="checking data")                
for batch in data_loader:                                                  
Â¦   t.update()                                                             
return
endsnippet

snippet mnist
import os
import torch
import pytorch_lightning as pl
from torch.nn import functional as F
from torch.utils.data import DataLoader, random_split
from torchvision.datasets.mnist import MNIST
from torchvision import transforms

########
# Data #
########

Dataset = MNIST


class DataModule(pl.LightningDataModule):

    def __init__(self,
            data_dir: str = './',
            batch_size: int = 32):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        # self.dims is returned when you call dm.size()
        # Setting default dims here because we know them.
        # Could optionally be assigned dynamically in dm.setup()
        self.dims = (1, 28, 28)
        self.num_classes = 10

    def prepare_data(self):
        # download
        MNIST(self.data_dir, train=True, download=True)
        MNIST(self.data_dir, train=False, download=True)

    def setup(self, stage=None):

        # Assign train/val datasets for use in dataloaders
        if stage == 'fit' or stage is None:
            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)
            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])

        # Assign test dataset for use in dataloader(s)
        if stage == 'test' or stage is None:
            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)

    def train_dataloader(self):
        return DataLoader(self.mnist_train, batch_size=self.batch_size)

    def val_dataloader(self):
        return DataLoader(self.mnist_val, batch_size=self.batch_size)

    def test_dataloader(self):
        return DataLoader(self.mnist_test, batch_size=self.batch_size)

#########
# Model #
#########

class Model(pl.LightningModule):
    def __init__(self, hidden_dim=128, learning_rate=1e-3):
        super().__init__()
        self.save_hyperparameters()

        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)
        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = torch.relu(self.l1(x))
        x = torch.relu(self.l2(x))
        return x

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('valid_loss', loss)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('train_loss', loss)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('test_loss', loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)


##################
# Config & Train #
##################

from easydict import EasyDict as edict

cfg = edict()
cfg.gpus = "0," 
cfg.batch_size = 1024
cfg.hidden_size = 128
cfg.lr = 0.0001


def train():
    pl.seed_everything(1234)

    # ------------
    # data
    # ------------
    dm = DataModule('', batch_size=cfg.batch_size)

    # ------------
    # model
    # ------------
    model = Model(cfg.hidden_size, cfg.lr)

    # ------------
    # training
    # ------------
    if torch.backends.mps.is_built():
        trainer = pl.Trainer(accelerator="mps")
    else:
        trainer = pl.Trainer(gpus=cfg.gpus)
    trainer.fit(model, dm)

    # ------------
    # testing
    # ------------
    trainer.test(model, datamodule=dm)
endsnippet
